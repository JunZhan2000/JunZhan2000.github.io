---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a second-year Master's student at the Fudan University, NLP Lab, supervised by Prof. [Xipeng Qiu](https://xpqiu.github.io/en.html). I graduated from Huazhong University of Science and Technology with a bachelor's degree in software engineering.

My current research focuses on unified multimodal LLMs and representation models. Feel free to contact me via email at [jzhan22@m.fudan.edu.cn](jzhan22@m.fudan.edu.cn).

# ğŸ”¥ News
- *2024.03*: &nbsp;ğŸ‰ğŸ‰ We release the data of [AnyGPT](https://junzhan2000.github.io/AnyGPT.github.io/). Welcome to STAR and FORK!
- *2024.02*: &nbsp;ğŸ¤–ğŸ¤– We release the paper of [AnyGPT](https://junzhan2000.github.io/AnyGPT.github.io/).

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2019</div><img src='images/fs.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FastSpeech: Fast, Robust and Controllable Text to Speech](https://papers.nips.cc/paper/8580-fastspeech-fast-robust-and-controllable-text-to-speech.pdf) \\
**Yi Ren**, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu

[**Project**](https://speechresearch.github.io/fastspeech/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

- AnyGPT is the first any-to-any multimodal LLM based on discrete representations.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects. Our work are promoted by more than 10 media and forums, such as [æœºå™¨ä¹‹å¿ƒ](https://www.jiqizhixin.com/articles/2024-03-04-10)ã€
</div>
</div>



<!-- 
<div class='paper-box'><div class='paper-box-image'><div><img src='images/anygpt-model1.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/pdf/2402.12226.pdf)

**Jun Zhan**, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang1, Xipeng Qiu


<div class='paper-box'><div class='paper-box-image'><div><img src='images/anygpt-model1.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](https://arxiv.org/pdf/2305.11000.pdf)

Dong Zhang, Shimin Li, Xin Zhang, **Jun Zhan**, Pengyu Wang, Yaqian Zhou, Xipeng Qiu -->



# ğŸ“– Educations
- *2022.09 - (now)*, Master, Fudan University, Shanghai.
- *2018.09 - 2022.06*, Undergraduate, Huazhong University of Science and Technology, Wuhan.